# -*- coding: utf-8 -*-
"""Natural Language Processing Assignment 1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xCMYNfu2i37IgoR4LPMBkEblkTn-IMWd

# Using handwritten Features

## Data Loading, Filtering and Preperation
"""

import torch

with open("city_inhabitant_data_final.csv" , "r") as file:
  lines = file.readlines()

samples = []
for line in lines:
  splitted = line.strip().split(",")
  city , inhabitant = splitted[0].strip().lower() , splitted[1].strip().lower()
  samples.append([city , inhabitant])


classes = set()
final_examples = dict()
for c,i in samples:
  cat = i.replace(c , "")
  classes.add(cat)
  if cat not in final_examples:
    final_examples[cat] = []
  if c not in final_examples[cat]:
    final_examples[cat].append(c)


filtered_examples = {}

for cat in final_examples:
  if len(final_examples[cat]) > 10:
    filtered_examples[cat] = final_examples[cat]



import json

with open("filteredExamples.json" , "w") as file:
  json.dump(filtered_examples , file)



print("Total Examples - " , sum([len(filtered_examples[x]) for x in filtered_examples]))

import json
with open("filteredExamples.json" , "r") as file:
  final_samples = json.load(file)


class_labels = {}
class_labels_to_class = {}
for i,clss in enumerate(final_samples):
  class_labels[clss] = i
  class_labels_to_class[i] = clss



inputsWithClasses = []
for clss in final_samples:
  for c in final_samples[clss]:
    inputsWithClasses.append([c , class_labels[clss] , clss])


distribution = {}
for cat in final_samples:
  distribution[cat] = len(final_samples[cat])

print("Distribution is \n" , distribution)


def convertCharToInt(char):
  if char == " ":
    return 26
  return ord(char) - ord('a')

def makeWordToVectorNaive(word , maxLen = 25):
  wordVector = []
  for i in range(maxLen):
    charVector = [0]*27
    if i < len(word):
      charIndex = convertCharToInt(word[i])
      if(0 <= charIndex < 27):
        charVector[charIndex] = 1
    wordVector.extend(charVector)
  return torch.tensor(wordVector).float()



input_x = []
input_y = []
for sample in inputsWithClasses:
  input_x.append(makeWordToVectorNaive(sample[0].lower()))
  input_y.append(sample[1])




input = torch.stack(input_x , dim = 0)

output = torch.nn.functional.one_hot(torch.tensor(input_y))

input = input.float()
output = output.float()

inputDimensions = 675

# from sklearn.decomposition import PCA


# decomposition = PCA(inputDimensions)
# decomposition.fit(input)
# input = torch.from_numpy(decomposition.transform(input)).float()


totalClasses = len(class_labels)
totalClasses

totalNumber = input.shape[0]
trainNumber = int(0.8*totalNumber)
remainingNumber = totalNumber - trainNumber

shuffledIndices = torch.randperm(totalNumber)
trainIndices = shuffledIndices[:trainNumber]
remainingIndices = shuffledIndices[trainNumber:]
len(trainIndices) , trainNumber , len(remainingIndices) , remainingNumber

train_input = input[trainIndices]
train_output = output[trainIndices]

train_input.shape , train_output.shape

remaining_input = input[remainingIndices]
remaining_output = output[remainingIndices]


testNumber = int(0.6*remainingNumber)
valNumber = remainingNumber - testNumber

shuffledIndices = torch.randperm(remainingNumber)
testIndices = shuffledIndices[:testNumber]
valIndices = shuffledIndices[testNumber:]

test_input = remaining_input[testIndices]
test_output = remaining_output[testIndices]
test_input.shape , test_output.shape


val_input = remaining_input[valIndices]
val_output = remaining_output[valIndices]
val_input.shape , val_output.shape



"""## Models

###Deep Learning Models
"""
from sklearn.metrics import precision_score, recall_score, f1_score

from sklearn.metrics import precision_score, recall_score, f1_score


def trainModel(epochs , optimizer , model , lossFunction , train_x , train_y , test_x , test_y):
  train_loss_array = []
  train_accuracy_array = []
  test_loss_array = []
  test_accuracy_array = []
  for epoch in range(epochs):
    train_out = model(train_x)
    loss_train = lossFunction(train_out , train_y)
    train_loss_array.append(loss_train.item())

    train_accuracy = (torch.argmax(train_out , dim = 1) == torch.argmax(train_y, dim = 1)).sum() / train_x.shape[0]
    train_accuracy_array.append(train_accuracy.item())

    test_out = model(test_x)
    loss_test = lossFunction(test_out , test_y)
    test_loss_array.append(loss_test.item())

    test_accuracy = (torch.argmax(test_out , dim = 1) == torch.argmax(test_y, dim = 1)).sum() / test_x.shape[0]
    test_accuracy_array.append(test_accuracy.item())
    
    
    optimizer.zero_grad()
    loss_train.backward()
    optimizer.step()

    train_pred = torch.argmax(train_out, dim=1)
    precision_train = precision_score(torch.argmax(train_y, dim=1), train_pred, average='weighted')
    recall_train = recall_score(torch.argmax(train_y, dim=1), train_pred, average='weighted')
    f1_train = f1_score(torch.argmax(train_y, dim=1), train_pred, average='weighted')

    test_pred = torch.argmax(test_out, dim=1)
    precision_test = precision_score(torch.argmax(test_y, dim=1), test_pred, average='weighted')
    recall_test = recall_score(torch.argmax(test_y, dim=1), test_pred, average='weighted')
    f1_test = f1_score(torch.argmax(test_y, dim=1), test_pred, average='weighted')

    if epoch == epochs-1:
      print()
      print(f"For epoch {epoch + 1} \n\t Train Loss is {loss_train} ,  Train Accuracy {train_accuracy * 100}\n\t Test Loss is {loss_test} , Test Accuracy is {test_accuracy * 100}")
      print(f"\t Train Precision : {precision_train * 100}, Recall: {recall_train * 100}, F1 Score: {f1_train * 100}")
      print(f"\t Test Precision: {precision_test * 100}, Recall: {recall_test * 100}, F1 Score: {f1_test * 100}")
      print("*****")

  finalDict = {}
  finalDict['train loss'] = train_loss_array
  finalDict['train accuracy'] = train_accuracy_array
  finalDict['test loss'] = test_loss_array
  finalDict['test accuracy'] = test_accuracy_array
  return finalDict


import torch.nn as nn
import torch.optim as optim

model = nn.Sequential(
    nn.Linear(inputDimensions , 100),
    nn.ReLU(),
    nn.Linear(100 , totalClasses),
    nn.Softmax()
)


optimizer = optim.Adam(model.parameters() , lr= 0.09)

metrics = trainModel(200 , optimizer , model , nn.CrossEntropyLoss() , train_input , train_output , test_input , test_output)


val_accuracy = (torch.argmax(model(val_input) , dim = 1) == torch.argmax(val_output, dim = 1)).sum() / val_input.shape[0]

print(f"Validation Accuracy : {val_accuracy*100}")



# import matplotlib.pyplot as plt

# history_dict = metrics

# loss_values = history_dict['train loss']
# val_loss_values = history_dict['test loss']
# epochs = range(1, len(loss_values) + 1)

# line1 = plt.plot(epochs, val_loss_values, label='Validation/Test Loss')
# line2 = plt.plot(epochs, loss_values, label='Training Loss')
# plt.setp(line1, linewidth=2.0, markersize=10.0)
# plt.setp(line2, linewidth=2.0, markersize=10.0)
# plt.xlabel('Epochs')
# plt.ylabel('Loss')
# plt.grid(True)
# plt.legend()
# plt.show()

# import matplotlib.pyplot as plt

# history_dict = metrics

# loss_values = history_dict['train accuracy']
# val_loss_values = history_dict['test accuracy']
# epochs = range(1, len(loss_values) + 1)

# line1 = plt.plot(epochs, val_loss_values, label='Validation/Test Accuracy')
# line2 = plt.plot(epochs, loss_values, label='Training Accuracy')
# plt.setp(line1, linewidth=2.0, markersize=10.0)
# plt.setp(line2, linewidth=2.0, markersize=10.0)
# plt.xlabel('Epochs')
# plt.ylabel('Accuracy')
# plt.grid(True)
# plt.legend()
# plt.show()


"""###Machine Learning Models"""

def genericTrainTestFunctionForMachineLearning(model , modelName , train_x , train_y , test_x , test_y , val_x , val_y):
  model.fit(train_x , train_y)
  
  # Calculate and print precision, recall, and F1 score
  test_predictions = model.predict(test_x)
  precision_test = precision_score(test_y, test_predictions, average='weighted')
  recall_test = recall_score(test_y, test_predictions, average='weighted')
  f1_test = f1_score(test_y, test_predictions, average='weighted')

  val_preds = model.predict(val_x)
  precision_val = precision_score(val_y, val_preds, average='weighted')
  recall_val = recall_score(val_y, val_preds, average='weighted')
  f1_val = f1_score(val_y, val_preds, average='weighted')


  print("Training Score for "+modelName+" is :" , model.score(train_x, train_y)*100)
  print("Test Score for "+modelName+" is :" , model.score(test_x, test_y)*100)
  print("Validation Score for "+modelName+" is :" , model.score(val_x, val_y)*100)

  print("Test")
  print(f"Precision for {modelName}: {precision_test*100}")
  print(f"Recall for {modelName}: {recall_test*100}")
  print(f"F1 Score for {modelName}: {f1_test*100}")

  print("Validation")
  print(f"Precision for {modelName}: {precision_val*100}")
  print(f"Recall for {modelName}: {recall_val*100}")
  print(f"F1 Score for {modelName}: {f1_val*100}")



"""####Decision Tree"""

import sklearn
from sklearn.tree import DecisionTreeClassifier

decisionTreeClassifier = DecisionTreeClassifier(random_state=1)

# Trainnng the classifier on the training set
genericTrainTestFunctionForMachineLearning(decisionTreeClassifier ,
                                           "Decision Tree" ,
                                           train_input,
                                           train_output,
                                           test_input,
                                           test_output,
                                           val_input,
                                           val_output)

"""#####Cross Validation Decision Tree"""

# from sklearn.model_selection import cross_val_score

# decisionTreeClassifierKCross = DecisionTreeClassifier(random_state=1)

# crossValidationScores = cross_val_score(decisionTreeClassifierKCross, input, output, cv=10)

# crossValidationScores.mean()*100

"""####Random Forest"""

from sklearn.ensemble import RandomForestClassifier

estimators = 3
randomForestClassifier = RandomForestClassifier(n_estimators=estimators)


genericTrainTestFunctionForMachineLearning(randomForestClassifier ,
                                           "Random Forest with "+str(estimators)+" estimators" ,
                                           train_input,
                                           train_output,
                                           test_input,
                                           test_output,
                                           val_input,
                                           val_output)

"""#####Cross Validation On Random Forest"""

# randomForestClassifier = RandomForestClassifier(n_estimators=3)
# randomForestClassifier_5Cross = cross_val_score(randomForestClassifier,input , output, cv=5)

# randomForestClassifier_5Cross.mean()*100

"""###Naive Bayes"""

from sklearn.naive_bayes import MultinomialNB

naiveBayesClassifier = MultinomialNB()

genericTrainTestFunctionForMachineLearning(naiveBayesClassifier ,
                                           "Multinomial Naive Bayes" ,
                                           train_input,
                                           torch.argmax(train_output ,dim=1),
                                           test_input,
                                           torch.argmax(test_output,dim=1),
                                           val_input,
                                           torch.argmax(val_output ,dim=1))

"""###SVM"""

from sklearn import svm

C = 1.0
kernal = 'rbf'
svm_rbf = svm.SVC(kernel=kernal, C=C)

genericTrainTestFunctionForMachineLearning(svm_rbf ,
                                           "S.V.M with rbf kernel" ,
                                           train_input,
                                           torch.argmax(train_output ,dim=1),
                                           test_input,
                                           torch.argmax(test_output,dim=1),
                                           val_input,
                                           torch.argmax(val_output ,dim=1))


C = 1.0
kernal = 'sigmoid'
svm_sigmoid = svm.SVC(kernel=kernal, C=C)


genericTrainTestFunctionForMachineLearning(svm_sigmoid ,
                                           "S.V.M with sigmoid kernel" ,
                                           train_input,
                                           torch.argmax(train_output ,dim=1),
                                           test_input,
                                           torch.argmax(test_output,dim=1),
                                           val_input,
                                           torch.argmax(val_output ,dim=1))



C = 1.0
kernal = 'poly'
svm_poly = svm.SVC(kernel=kernal, C=C)


genericTrainTestFunctionForMachineLearning(svm_poly ,
                                           "S.V.M with polynomial kernel" ,
                                           train_input,
                                           torch.argmax(train_output ,dim=1),
                                           test_input,
                                           torch.argmax(test_output,dim=1),
                                           val_input,
                                           torch.argmax(val_output ,dim=1))



"""##Ensemble Method"""
class EnsambleMethod:
  def __init__(self , neuralModel , decisionTreeModel , randomForestModel, naiveBayesModel, svmModel):
    self.neuralModel = neuralModel
    self.decisionTreeModel=decisionTreeModel
    self.randomForestModel=randomForestModel
    self.naiveBayesModel=naiveBayesModel
    self.svmModel=svmModel

  def predictClass(self , test):
    nn = torch.argmax(self.neuralModel(test) , dim = 1)
    dt = torch.argmax(torch.from_numpy(self.decisionTreeModel.predict(test)) , dim=1)
    rf = torch.argmax(torch.from_numpy(self.randomForestModel.predict(test)) , dim=1)
    nb = torch.from_numpy(self.naiveBayesModel.predict(test))
    svm = torch.from_numpy(self.svmModel.predict(test))

    finalStack = torch.stack([nn,dt,rf,nb,svm])
    return torch.mode(finalStack , dim=0).values


  def getAccuracy(self , test_input , test_output):
    predicted = self.predictClass(test_input)
    print("Accuracy of Ensemble Method is : " , (predicted == test_output).float().mean().item()*100)
    return

  def getMetrics(self , test_input , test_output):
    self.getAccuracy(test_input , test_output)
    test_pred = self.predictClass(test_input)
    precision = precision_score(test_output, test_pred, average='weighted')
    recall = recall_score(test_output, test_pred, average='weighted')
    f1 = f1_score(test_output, test_pred, average='weighted')
    print(f"Precision : {precision*100}")
    print(f"Recall :{recall*100}")
    print(f"F1 Score :{f1*100}")



ensemble = EnsambleMethod(model,
                          decisionTreeClassifier,
                          randomForestClassifier,
                          naiveBayesClassifier,
                          svm_rbf)
ensemble.getAccuracy(train_input , torch.argmax(train_output , dim=1))
ensemble.getAccuracy(test_input , torch.argmax(test_output , dim=1))
ensemble.getAccuracy(val_input , torch.argmax(val_output , dim=1))

ensemble.getMetrics(train_input , torch.argmax(train_output , dim=1))
ensemble.getMetrics(test_input , torch.argmax(test_output , dim=1))
ensemble.getMetrics(val_input , torch.argmax(val_output , dim=1) )



import numpy as np

def getNeuralNetPrediction(city):
  cityVector = makeWordToVectorNaive(city)
  nnOut = torch.argmax(model(cityVector)).item()
  nnPred = city + class_labels_to_class[nnOut]
  return nnPred


def getDecisionTreePrediction(city):
  cityVector = makeWordToVectorNaive(city)
  dtOut = torch.argmax(torch.from_numpy(decisionTreeClassifier.predict(cityVector.reshape(1,-1))) , dim=1).item()
  dtPred = city+class_labels_to_class[dtOut]
  return dtPred


def getRandomForestPrediction(city):
  cityVector = makeWordToVectorNaive(city)
  rtOut = torch.argmax(torch.from_numpy(randomForestClassifier.predict(cityVector.reshape(1,-1))) , dim=1).item()
  rtPred = city+class_labels_to_class[rtOut]
  return rtPred


def getNaiveBayesPrediction(city):
  cityVector = makeWordToVectorNaive(city)
  nbOut = torch.from_numpy(naiveBayesClassifier.predict(cityVector.reshape(1,-1))).item()
  nbPred = city+class_labels_to_class[nbOut]
  return nbPred

def getSVMPrediction(city):
  cityVector = makeWordToVectorNaive(city)
  svmOut = torch.from_numpy(svm_rbf.predict(cityVector.reshape(1,-1))).item()
  svmPred = city+class_labels_to_class[svmOut]
  return svmPred


def getEnsemblePrediction(city):
  cityVector = makeWordToVectorNaive(city)
  enOut = ensemble.predictClass(cityVector.reshape(1,-1)).item()
  enPred = city+class_labels_to_class[enOut]
  return enPred




def predictDenonym(city):

  print("Neural Network Output : " , getNeuralNetPrediction(city))


  print("Decision Tree Output : " , getDecisionTreePrediction(city))



  print("Random Forest Output : " ,getRandomForestPrediction(city) )



  print("Naive Bayes Output : " , getNaiveBayesPrediction(city))


  print("S.V.M Output : " , getSVMPrediction(city))


  print("Ensemble Method Output : " , getEnsemblePrediction(city))


