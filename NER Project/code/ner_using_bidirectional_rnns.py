# -*- coding: utf-8 -*-
"""NER_using_Bidirectional_RNNs.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ZWXbYru32LbK17RoJm3pWqxYCCHfMT8Q
"""

# .csv
# !wget https://storage.googleapis.com/drive-bulk-export-anonymous/20231128T134032.039Z/4133399871716478688/f3fcf720-bf7d-47c7-a0fd-fb409bc89c7d/1/ee02ecc1-7e57-4122-b315-e27a7bbcaaca?authuser

# .npy
# !wget https://storage.googleapis.com/drive-bulk-export-anonymous/20231128T134246.045Z/4133399871716478688/a87a6b96-913c-4bf8-af7c-c81470cd1661/1/94ebb924-d84e-4aed-9569-69ee466fd809?authuser

# .emb
# !wget https://storage.googleapis.com/drive-bulk-export-anonymous/20231128T134738.427Z/4133399871716478688/dd54b441-0086-4d01-ba83-5212d16fe1f0/1/ff7a5561-78cd-408d-a1fb-b5a0200cc18e?authuser

# datpp.npy
# !wget https://storage.googleapis.com/drive-bulk-export-anonymous/20231128T135501.188Z/4133399871716478688/3cc1b8f7-53e1-4956-9006-2bdc0d497461/1/b5485f1f-13b5-4195-9312-49df6f2393de?authuser

# !unzip test.zip

# import numpy as np

# datapp = np.load("datapp.npy" , allow_pickle=True)

# datapp

# NER_Array = []

# for dic in datapp:
   # NER_Array.append(dic)

import pickle
NER_Array = pickle.load(open("/content/data_without_full_sentence_old.pkl" , "rb"))

len(NER_Array)

"""#Data Filtering

"""

sentences = [x['Sentence'] for x in NER_Array]

sentences

tags = [x["Tags"] for x in NER_Array]

tags

max_sentence_length = max(len(sentence) for sentence in sentences)
print('biggest sentence has {} words'.format(max_sentence_length))

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pyplot as plt
# %matplotlib inline
plt.hist([len(sen) for sen in sentences], bins= 50)
plt.show()

filtered_NER_Array = [x for x in NER_Array if len(x['Sentence']) < 11]

len(filtered_NER_Array)

sentences = [x['Sentence'] for x in filtered_NER_Array]

tags = [x["Tags"] for x in filtered_NER_Array]

max_sentence_length = max(len(sentence) for sentence in sentences)
print('biggest sentence has {} words'.format(max_sentence_length))

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pyplot as plt
# %matplotlib inline
plt.hist([len(sen) for sen in sentences], bins= 50)
plt.show()

words_arr = []
for sent in sentences:
  for word in sent:
    words_arr.append(word)

words = set(words_arr)
words.add("<PAD>")
words.add("<UNK>")
number_words = len(words)
number_words

tags_arr = []
for tag_lst in tags:
  for tag in tag_lst:
    tags_arr.append(tag)

total_tags = set(tags_arr)
number_tags = len(total_tags)
number_tags

total_tags

# def get_sentences_with_tags(sentences , tags):
#   sentence_and_tags = []
#   for sentence_lst , tag_lst in zip(sentences , tags):
#     sentence_and_tag = []
#     for word , tag in zip(sentence_lst , tag_lst):
#       sentence_and_tag.append((word,tag))
#     sentence_and_tags.append(sentence_and_tag)

#   return sentence_and_tags

# sentence_and_tags = get_sentences_with_tags(sentences , tags)

# sentence_and_tags

words2index = {w:i for i,w in enumerate(words)}
index2words = {i:w for i,w in enumerate(words)}
tags2index = {t:i for i,t in enumerate(total_tags)}
index2tags = {i:t for i,t in enumerate(total_tags)}

words2index["<PAD>"] , words2index["<UNK>"]

words2index

import pickle


def save_dict(dictionary,path):
  with open(path , "wb") as file:
    pickle.dump(dictionary , file)
  print("Saved at " , path)

def load_dict(path):
  dictionary = {}
  with open(path , "rb") as file:
    dictionary = pickle.load(file)
  print("Loaded from " , path)
  return dictionary

save_dict(words2index , "words2index_without_10_f.pkl")

save_dict(index2words , "index2words_without_10_f.pkl")

save_dict(tags2index , "tags2index_without_10_f.pkl")

save_dict(index2tags , "index2tags_without_10_f.pkl")





tags2index

max_length = max_sentence_length
max_length

# max_length = max_sentence_length

# new_sentences = []

# for sentence in sentences:
#   new_seq = []
#   for i in range(max_length):
#     if i<len(sentence):
#       new_seq.append(sentence[i])
#     else:
#       new_seq.append("<PAD>")
#   new_sentences.append(new_seq)

# new_sentences[15]

from keras.preprocessing.sequence import pad_sequences
from keras.utils import to_categorical



new_tags = [[tags2index[tag] for tag in tag_list] for tag_list in tags]
new_tags = pad_sequences(maxlen=max_length, sequences=new_tags, padding="post", value=tags2index["O"])
new_tags_one_hot = [to_categorical(i, num_classes=number_tags) for i in new_tags]

input_sentences = [[words2index[word] for word in sentense ]for sentense in sentences]
# add padding for <PAD>
input_sentences = pad_sequences(maxlen = max_length , sequences = input_sentences , padding='post', value = words2index["<PAD>"])

print(sentences[7])
for idx in input_sentences[7]:
  print(index2words[idx] , sep=" ")

new_tags[6] , new_tags_one_hot[6] , tags[6]

from sklearn.model_selection import train_test_split



train_on_sentences, sentences_validation, train_on_tags, tags_validation = train_test_split(input_sentences, new_tags_one_hot, test_size=0.1, random_state=5)

train_sentences, test_sentences, train_tags, test_tags = train_test_split(train_on_sentences, train_on_tags, test_size=0.2, random_state=8)

# # import tensorflow as tf
# import tensorflow_hub as hub
# from tensorflow.compat.v1.keras import backend as K
# sess = tf.Session()
# K.set_session(sess)

# elmo_model = hub.Module("https://tfhub.dev/google/elmo/2", trainable=True)
# sess.run(tf.global_variables_initializer())
# sess.run(tf.tables_initializer())

# batch_size = 16

# def ElmoEmbedding(x):
#     return elmo_model(inputs={
#                             "tokens": tf.squeeze(tf.cast(x, tf.string)),
#                             "sequence_len": tf.constant(batch_size*[max_length])
#                       },
#                       signature="tokens",
#                       as_dict=True)["elmo"]

# from keras.models import Model
# # from keras.layers.merge import add
# from keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional, Lambda, Input , add



# input_text = Input(shape=(max_length,), dtype=tf.string)
# embedding = Lambda(ElmoEmbedding, output_shape=(max_length, 1024))(input_text)
# x = Bidirectional(LSTM(units=512, return_sequences=True,
#                        recurrent_dropout=0.2, dropout=0.2))(embedding)
# x_rnn = Bidirectional(LSTM(units=512, return_sequences=True,
#                            recurrent_dropout=0.2, dropout=0.2))(x)
# x = add([x, x_rnn])  # residual connection to the first biLSTM
# out = TimeDistributed(Dense(number_tags, activation="softmax"))(x)

train_sentences.shape , test_sentences.shape , sentences_validation.shape

from keras import Model, Input
from keras.layers import LSTM, Embedding, Dense
from keras.layers import TimeDistributed, SpatialDropout1D, Bidirectional , add

def get_model(max_length ,number_words , number_tags, num_units , recurrent_dropout=0.2 , dropout = 0.1):
    input_word = Input(shape = (max_length,))

    model = Embedding(input_dim = number_words, output_dim = max_length, input_length = max_length)(input_word)

    model = SpatialDropout1D(0.1)(model)

    rnn1 = Bidirectional(LSTM(units = num_units, return_sequences=True, recurrent_dropout=recurrent_dropout , dropout = dropout))(model)

    rnn2 = Bidirectional(LSTM(units = num_units, return_sequences=True, recurrent_dropout= recurrent_dropout , dropout =  dropout))(rnn1)

    model = add([rnn1 , rnn2])

    out = TimeDistributed(Dense(number_tags , activation='softmax'))(model)

    model = Model(input_word,out)

    print(model.summary())

    return model

from keras.callbacks import ModelCheckpoint

checkpoint_path = "model1/model.ckpt"

checkpoint_callback = ModelCheckpoint(filepath=checkpoint_path,
                                                 save_weights_only=True,
                                                 verbose=1)

model = get_model(max_length , number_words , number_tags , 256)

model.compile(optimizer="adam",loss='categorical_crossentropy',metrics=['accuracy'])

import numpy as np
history = model.fit(
    train_sentences ,
    np.array(train_tags),
    batch_size = 256,
    epochs = 20,
    validation_data=(test_sentences , np.array(test_tags)),
    callbacks = [checkpoint_callback],
    verbose = 1
)

vals = model.evaluate(train_sentences , np.array(train_tags))
print(vals)

model.save("model-20-epochs_without_10_f_256.keras")



from keras.models import load_model
loaded_model = load_model("model-20-epochs_without_10_f.keras")

loaded_model.evaluate(sentences_validation , np.array(tags_validation))

true = np.argmax(np.array(test_tags), axis=-1)
preds= np.argmax(model.predict(np.array(test_sentences)), axis=-1)

loaded_model.evaluate(sentences_validation , np.array(tags_validation))

test_sentences[2]

# y_true = np.argmax(np.array(test_tags), axis=-1)[i]
# y_true

i = np.random.randint(0, sentences_validation.shape[0])
print([sentences_validation[i]])
p = model.predict(np.array([sentences_validation[i]]))
p = np.argmax(p, axis=-1)

y_true = np.argmax(np.array(tags_validation), axis=-1)[i]
print("{:15}{:5}\t {}\n".format("Word","True","Pred"))
print("-"*30)
for w,true,pred in zip(sentences_validation[i], y_true, p[0]):
  print("{:15} {:5}\t {}".format(index2words[w], index2tags[true],index2tags[pred]))

import numpy as np
def predict_NER_Tags(model , sentence , index2tags):

  input_sentence = [words2index[word] for word in sentence]
  # add padding for <PAD>
  input_sentence = pad_sequences(maxlen = max_length , sequences = [input_sentence] , padding='post', value = words2index["<PAD>"])[0]
  predictions = model.predict(np.array([input_sentence]))
  preds = np.argmax(predictions, axis=-1)

  for w,pred in zip(sentence, preds[0]):

    print("{:15} {:5}\t".format(w,index2tags[pred]))

# k = np.random.randint(0, len(sentences))
new_ind2tag = load_dict("/content/index2tags_without_10_f.pkl")
predict_NER_Tags(model , sentences[k] , new_ind2tag)

index2words[w]











